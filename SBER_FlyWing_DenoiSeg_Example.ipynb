{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenoiSeg Example: Fly Wing\n",
    "This is an example notebook which illustrates how DenoiSeg should be trained. In this notebook we use a membrane labeled developing Fly Wing dataset from our collaborators. We already split the data into train and test images. From the train images we then extracted 1428 training and 252 validation patches of size 128x128. The test set contains 50 images of size 512x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n",
      "  Downloading h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.1/202.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sasha/anaconda3/envs/denoiseg/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.16.2)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, MarkupSafe, keras, importlib-metadata, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, markdown, h5py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.2.0 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 idna-3.4 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 werkzeug-2.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m ndimage\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdenoiseg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoiSeg, DenoiSegConfig\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdenoiseg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m combine_train_test_data, shuffle_train_data, augment_data\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdenoiseg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseg_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/denoiseg/lib/python3.9/site-packages/denoiseg/models/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# imports\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdenoiseg_config\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoiSegConfig\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdenoiseg_standard\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoiSeg\n",
      "File \u001b[0;32m~/anaconda3/envs/denoiseg/lib/python3.9/site-packages/denoiseg/models/denoiseg_config.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mK\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcsbdeep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _raise, axes_check_and_normalize, axes_dict, backend_channels_last\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Here we are just importing some libraries which are needed to run this notebook.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "from denoiseg.models import DenoiSeg, DenoiSegConfig\n",
    "from denoiseg.utils.misc_utils import combine_train_test_data, shuffle_train_data, augment_data\n",
    "from denoiseg.utils.seg_utils import *\n",
    "from denoiseg.utils.compute_precision_threshold import measure_precision\n",
    "from denoiseg.utils.denoiseg_data_preprocessing import generate_patches_from_list\n",
    "\n",
    "from csbdeep.utils import plot_history\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and  Data Loading\n",
    "We created three versions of this dataset by adding Gaussian noise with zero mean and standard deviations 10 and 20. The dataset are marked with the suffixes n0, n10 and n20 accordingly.\n",
    "\n",
    "In the next cell you can choose which `noise_level` you would like to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the noise level you would like to look at:\n",
    "# Values: 'n0', 'n10', 'n20'\n",
    "# noise_level = 'n20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for our data\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('data')\n",
    "    \n",
    "if noise_level == 'n0':\n",
    "    link = 'https://zenodo.org/record/5156991/files/Flywing_n0.zip?download=1'\n",
    "elif noise_level == 'n10':\n",
    "    link = 'https://zenodo.org/record/5156993/files/Flywing_n10.zip?download=1'\n",
    "elif noise_level == 'n20':\n",
    "    link = 'https://zenodo.org/record/5156995/files/Flywing_n20.zip?download=1'\n",
    "else:\n",
    "    print('This noise level does not exist for this dataset.')\n",
    "\n",
    "# check if data has been downloaded already\n",
    "zipPath=\"data/Flywing_{}.zip\".format(noise_level)\n",
    "if not os.path.exists(zipPath):\n",
    "    #download and unzip data\n",
    "    data = urllib.request.urlretrieve(link, zipPath)\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of the training images\n",
    "trainval_data =  np.load('/home/sasha/WSLProjects/denoising/DenoiSeg/scripts/reproducibility/dataprep/data/train/train_data.npz')\n",
    "\n",
    "train_images = trainval_data['X_train'].astype(np.float32)\n",
    "train_masks = trainval_data['Y_train']\n",
    "val_images = trainval_data['X_val'].astype(np.float32)\n",
    "val_masks = trainval_data['Y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train images: {}\".format(train_images.shape))\n",
    "print(\"Shape of train masks: {}\".format(train_masks.shape))\n",
    "print(\"Shape of validation images: {}\".format(val_images.shape))\n",
    "print(\"Shape of validation: {}\".format(val_masks.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Amounts of Annotated Training Data\n",
    "With DenoiSeg we present a solution to train deep neural networks if only few annotated ground truth segmentations are available. We simulate such a scenario by zeroing out all but a fraction of the available training data. In the next cell you can specify the percentage of training images for which ground truth annotations are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of annotated training images.\n",
    "# Values: 0.0 (no annotated images) to total number of training images (all images have annotations)\n",
    "number_of_annotated_training_images = 5\n",
    "assert number_of_annotated_training_images >= 0.0 and number_of_annotated_training_images <=train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed to shuffle training data (annotated GT and raw image pairs).\n",
    "seed = 1 \n",
    "\n",
    "# First we shuffle the training images to remove any bias.\n",
    "X_shuffled, Y_shuffled = shuffle_train_data(train_images, train_masks, random_seed=seed)\n",
    "\n",
    "# Here we convert the number of annotated images to be used for training as percentage of available training data.\n",
    "percentage_of_annotated_training_images = float((number_of_annotated_training_images/train_images.shape[0]) * 100.0)\n",
    "assert percentage_of_annotated_training_images >= 0.0 and percentage_of_annotated_training_images <= 100.0\n",
    "\n",
    "# Here we zero out all training images which are not part of the \n",
    "# selected percentage.\n",
    "X_frac, Y_frac = zero_out_train_data(train_images, train_masks, fraction=percentage_of_annotated_training_images)\n",
    "\n",
    "# Here we generate patches from images and apply augmentation\n",
    "X_final, Y_final = generate_patches_from_list([X_frac], [Y_frac], augment=True, shuffle=False, shape=(128, 128))\n",
    "X_val_final, Y_val_final = generate_patches_from_list([val_images], [val_masks], augment=False, shape=(128, 128))\n",
    "\n",
    "X_final = X_final[... ,np.newaxis]\n",
    "Y_final = convert_to_oneHot(Y_final, n_classes=3)\n",
    "\n",
    "X_val_final = X_val_final[... ,np.newaxis]\n",
    "Y_val_final = convert_to_oneHot(Y_val_final, n_classes=3)\n",
    "\n",
    "print(\"Shape of train images after augmentation: {}\".format(X_final.shape))\n",
    "print(\"Shape of train masks after augmentation: {}\".format(Y_final.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at a single sample. In the first column we show the input image, in the second column the background segmentation, in the third column the foreground segmentation and in the last column the border segmentation.\n",
    "\n",
    "With the parameter `sample` you can choose different training patches. You will notice that not all of them have a segmentation ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 0\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(X_final[sample,...,0])\n",
    "plt.axis('off')\n",
    "plt.title('Raw validation image')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(Y_final[sample,...,0], vmin=0, vmax=1, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('1-hot encoded background')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(Y_final[sample,...,1], vmin=0, vmax=1, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('1-hot encoded foreground')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(Y_final[sample,...,2], vmin=0, vmax=1, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('1-hot encoded border')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 128\n",
    "train_steps_per_epoch = min(400, max(int(X_final.shape[0]/train_batch_size), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next cell, you can choose how much relative importance (weight) to assign to denoising\n",
    "and segmentation tasks by choosing appropriate value for denoiseg_alpha (between 0 and 1; with 0 being\n",
    "only segmentation and 1 being only denoising. Here we choose denoiseg_alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = DenoiSegConfig(X_final, unet_kern_size=3, n_channel_in=1, n_channel_out=4, relative_weights = [1.0,1.0,5.0],\n",
    "                      train_steps_per_epoch=train_steps_per_epoch, train_epochs=10,\n",
    "                      batch_norm=True, train_batch_size=train_batch_size, unet_n_first=32,\n",
    "                      unet_n_depth=4, denoiseg_alpha=0.5, train_tensorboard=False)\n",
    "\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'SBER'\n",
    "basedir = 'models'\n",
    "model = DenoiSeg(conf, model_name, basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.train(X_final, Y_final, (X_val_final, Y_val_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Threshold Value\n",
    "The network predicts 4 output channels:\n",
    "1. The denoised input.\n",
    "2. The foreground likelihoods.\n",
    "3. The background likelihoods.\n",
    "4. The border likelihoods.\n",
    "\n",
    "We will threshold the foreground prediction image to obtain object segmentations. The optimal threshold is determined on the validation data. Additionally we can optimize the threshold for a given measure. In this case we choose the Average Precision (AP) measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold, val_score = model.optimize_thresholds(val_images.astype(np.float32), val_masks, measure=measure_precision(), axes='YX')\n",
    "\n",
    "print(\"The higest score of {} is achieved with threshold = {}.\".format(np.round(val_score, 3), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "Finally we load the test data and run the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  np.load('/home/sasha/WSLProjects/denoising/DenoiSeg/scripts/reproducibility/dataprep/data/test/test_data.npz', allow_pickle=True)\n",
    "test_images = test_data['X_test']\n",
    "test_masks = test_data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_denoised, predicted_images, precision_result, _ = model.predict_denoised_label_masks(resval_x,\n",
    "                                                                                               resval_y,\n",
    "                                                                                               axes='YX',\n",
    "                                                                                               threshold=threshold,\n",
    "                                                                                               measure=measure_precision(),\n",
    "                                                                                               n_tiles=4)\n",
    "\n",
    "print(\"Average precision over all test images with threshold = {} is {}.\".format(threshold, np.round(precision_result, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sl = 1\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(test_images[sl])\n",
    "plt.title(\"Raw image\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(predicted_denoised[sl])\n",
    "plt.title(\"Predicted denoised image\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(predicted_images[sl])\n",
    "plt.title(\"Predicted foreground segmentation\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(test_masks[sl])\n",
    "plt.title(\"Ground truth segmentation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export your model for Fiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_TF(name='DenoiSeg - SBER Example', \n",
    "                description='This is the 2D DenoiSeg example trained on SBER dataset', \n",
    "                authors=[\"SBER\"],\n",
    "                test_img=X_val[0,...,0], axes='YX',\n",
    "                patch_shape=(128, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('denoiseg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaff8ce660795c350e82a9023d0aaa467e8b7101b67765ba5fbe8080eb1ae6b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
